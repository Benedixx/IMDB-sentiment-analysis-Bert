{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS TUF GAMING\\AppData\\Local\\Temp\\ipykernel_17452\\3672979764.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "d:\\Anaconda\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\NLP\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'dataset\\IMDB Dataset.csv'\n",
    "df = pd.read_csv(file_name, delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review'].values\n",
    "labels = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-uncased\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "\tstop_words = stopwords.words('english')\n",
    "\n",
    "\timp_words = []\n",
    "\n",
    "\tfor word in str(text).split():\n",
    "\t\tword = word.lower()\n",
    "\n",
    "\t\tif word not in stop_words:\n",
    "\t\t\timp_words.append(word)\n",
    "\n",
    "\toutput = \" \".join(imp_words)\n",
    "\n",
    "\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(review):\n",
    "    review= review.lower()\n",
    "    review= remove_stopwords(review)\n",
    "    review= re.sub(r'[^a-zA-Z\\s]','',review)\n",
    "    review= re.sub(r'\\s+',' ',review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASUS TUF\n",
      "[nltk_data]     GAMING\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "reviews= [preprocessing(review) for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one reviewers mentioned watching oz episode hooked right exactly happened mebr br the first thing struck oz brutality unflinching scenes violence set right word go trust me show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br it called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br i would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz mess around first episode ever saw struck nasty surreal say ready it watched more developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n"
     ]
    }
   ],
   "source": [
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: one reviewers mentioned watching oz episode hooked right exactly happened mebr br the first thing struck oz brutality unflinching scenes violence set right word go trust me show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br it called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br i would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz mess around first episode ever saw struck nasty surreal say ready it watched more developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side\n",
      "label positive\n",
      "Tokenized: ['one', 'review', '##ers', 'mentioned', 'watching', 'oz', 'episode', 'hook', '##ed', 'right', 'exactly', 'happened', 'me', '##br', 'br', 'the', 'first', 'thing', 'struck', 'oz', 'brutal', '##ity', 'un', '##fl', '##in', '##ching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'me', 'show', 'fai', '##nt', 'heart', '##ed', 'tim', '##id', 'show', 'pull', '##s', 'punch', '##es', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', '##br', 'br', 'it', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'pen', '##iten', '##tary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'front', '##s', 'face', 'in', '##wards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', '##ary', '##ans', 'muslims', 'gang', '##stas', 'latinos', 'christians', 'italians', 'irish', 'more', '##so', 'sc', '##uf', '##fl', '##es', 'death', 'stare', '##s', 'dod', '##gy', 'dealing', '##s', 'sha', '##dy', 'agreements', 'never', 'far', 'away', '##br', 'br', 'i', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'char', '##m', 'forget', 'romance', '##oz', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nas', '##ty', 'sur', '##real', 'say', 'ready', 'it', 'watched', 'more', 'developed', 'taste', 'oz', 'got', 'acc', '##ust', '##ome', '##d', 'high', 'levels', 'graphic', 'violence', 'violence', 'in', '##jus', '##tice', 'cr', '##ook', '##ed', 'guards', 'who', '##ll', 'sold', 'nickel', 'in', '##mates', 'who', '##ll', 'kill', 'order', 'get', 'away', 'it', 'well', 'manner', '##ed', 'middle', 'class', 'in', '##mates', 'turned', 'prison', 'bit', '##ches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfort', '##able', 'un', '##com', '##fort', '##able', 'viewing', '##tha', '##ts', 'get', 'touch', 'dark', '##er', 'side']\n",
      "Token IDs: [10399, 12893, 11122, 24838, 58997, 17704, 12293, 27702, 10390, 12873, 56938, 35613, 10525, 47869, 18710, 10103, 10403, 21973, 41971, 17704, 63573, 12705, 10119, 67533, 10262, 49469, 21975, 22366, 10486, 12873, 12290, 11335, 19367, 10525, 11391, 29952, 10368, 13645, 10390, 12268, 11296, 11391, 59289, 10107, 60951, 10165, 88822, 31380, 14833, 22366, 34023, 17045, 11868, 12290, 47869, 18710, 10197, 11723, 17704, 62395, 13613, 31429, 19874, 15636, 10652, 27435, 24519, 80394, 54821, 20886, 88374, 10575, 23998, 13905, 18357, 22596, 20464, 13175, 10107, 12828, 10104, 45113, 77325, 11053, 46558, 10252, 10575, 11402, 11260, 17028, 15855, 42344, 14177, 21604, 40026, 47677, 92227, 15925, 10772, 11513, 16427, 23877, 67533, 10165, 11901, 21638, 10107, 17199, 18682, 64757, 10107, 51252, 12599, 82015, 13362, 12218, 13795, 47869, 18710, 151, 11008, 16497, 11659, 33426, 11391, 10875, 17772, 18984, 15503, 33692, 58884, 31897, 17202, 33108, 36560, 40111, 58884, 55804, 10150, 58884, 21419, 19615, 38234, 12096, 10403, 12293, 15765, 16289, 41971, 12621, 11406, 10344, 37792, 16497, 27080, 10197, 84447, 10772, 14906, 44960, 17704, 15517, 85312, 18336, 22639, 10163, 11053, 20957, 45706, 22366, 22366, 10104, 30090, 35632, 24590, 40425, 10390, 42429, 10488, 11552, 15434, 60497, 10104, 45271, 10488, 11552, 20082, 11970, 13168, 13795, 10197, 11327, 21208, 10390, 13470, 12728, 10104, 45271, 20900, 18357, 16464, 17919, 10875, 23051, 11902, 30504, 18357, 16277, 58997, 17704, 10431, 13706, 66493, 13356, 10119, 21567, 26120, 13356, 91355, 26010, 10870, 13168, 24965, 14471, 10177, 12029]\n"
     ]
    }
   ],
   "source": [
    "review= reviews[0]\n",
    "\n",
    "print('original:', review)\n",
    "print('label', labels[1])\n",
    "\n",
    "print('Tokenized:', tokenizer.tokenize(review))\n",
    "\n",
    "print('Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: wonderful little production br br the filming technique unassuming oldtimebbc fashion gives comforting sometimes discomforting sense realism entire piece br br the actors extremely well chosen michael sheen has got polari voices pat too truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great masters comedy life br br the realism really comes home little things fantasy guard which rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwells murals decorating every surface terribly well done\n",
      "Token IDs: [101, 49108, 11975, 11961, 18710, 18710, 10103, 48924, 20850, 26354, 24667, 10285, 11737, 23710, 14638, 10261, 23589, 24442, 66493, 10285, 16933, 14420, 10150, 26120, 10285, 15178, 89441, 18771, 19401, 17415, 18710, 18710, 10103, 26826, 36231, 11327, 26249, 10721, 10572, 10142, 10438, 15517, 30384, 10116, 34362, 15608, 14666, 69434, 11811, 12381, 43615, 22523, 59480, 71158, 34753, 12718, 42213, 75347, 11327, 25840, 58997, 12555, 37348, 11183, 10563, 13485, 15487, 17415, 13361, 15836, 11961, 10399, 11838, 19445, 18422, 10287, 18710, 18710, 10103, 89441, 18771, 25165, 18408, 11402, 11975, 17994, 18532, 21211, 10359, 16571, 11868, 16516, 16121, 21263, 18973, 25164, 31021, 27820, 23996, 17173, 19190, 15178, 10107, 19772, 21975, 43126, 66221, 11672, 10116, 16349, 22655, 19772, 22803, 11672, 10116, 16349, 10107, 76640, 10107, 75219, 35437, 13667, 15482, 12555, 45795, 10563, 11327, 19123, 102]\n"
     ]
    }
   ],
   "source": [
    "input_idx= []\n",
    "\n",
    "for review in reviews:\n",
    "    encoded_review= tokenizer.encode(\n",
    "        review,\n",
    "        add_special_tokens= True\n",
    "    )\n",
    "    input_idx.append(encoded_review)\n",
    "\n",
    "print('Original:', reviews[1])\n",
    "print('Token IDs:', input_idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  1\n"
     ]
    }
   ],
   "source": [
    "label_idx= []\n",
    "\n",
    "for label in labels:\n",
    "    if label == 'positive':\n",
    "        encoded_label= 1\n",
    "    else:\n",
    "        encoded_label=0\n",
    "    label_idx.append(encoded_label)\n",
    "\n",
    "print('label: ', label_idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = np.array(label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding/truncating all sentences to 128 values\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "print(\"Padding/truncating all sentences to %d values\" % MAX_LEN)\n",
    "print('Padding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = pad_sequences(input_idx, maxlen=MAX_LEN, dtype='long', value=0, truncating='post', padding='post')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = []\n",
    "\n",
    "for sent in input_ids:\n",
    "  att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "  attention_mask.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_labels, test_labels = train_test_split(input_ids,\n",
    "                                                                      label_idx,\n",
    "                                                                      random_state=2017,\n",
    "                                                                      test_size=0.1)\n",
    "train_mask, test_mask, _, _ = train_test_split(attention_mask,\n",
    "                                               label_idx,\n",
    "                                               random_state=2017,\n",
    "                                               test_size=0.1)\n",
    "\n",
    "train_input, validation_input, train_labels, validation_labels = train_test_split(train_input,\n",
    "                                                                                  train_labels,\n",
    "                                                                                  random_state=2018,\n",
    "                                                                                  test_size=0.15)\n",
    "train_mask, validation_mask, _, _ = train_test_split(train_mask,\n",
    "                                                     train_mask,\n",
    "                                                     random_state=2018,\n",
    "                                                     test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Train ==\n",
      "Input:  (38250, 128)\n",
      "Label:  (38250,)\n",
      "Mask:  (38250, 128)\n",
      "\n",
      "== Validation ==\n",
      "Input:  (6750, 128)\n",
      "Label:  (6750,)\n",
      "Mask:  (6750, 128)\n",
      "\n",
      "== Test ==\n",
      "Input:  (5000, 128)\n",
      "Label:  (5000,)\n",
      "Mask:  (5000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"== Train ==\")\n",
    "print(\"Input: \", train_input.shape)\n",
    "print(\"Label: \", train_labels.shape)\n",
    "print(\"Mask: \", np.array(train_mask).shape)\n",
    "\n",
    "print(\"\\n== Validation ==\")\n",
    "print(\"Input: \", validation_input.shape)\n",
    "print(\"Label: \", validation_labels.shape)\n",
    "print(\"Mask: \", np.array(validation_mask).shape)\n",
    "\n",
    "print(\"\\n== Test ==\")\n",
    "print(\"Input: \", test_input.shape)\n",
    "print(\"Label: \", test_labels.shape)\n",
    "print(\"Mask: \", np.array(test_mask).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = torch.tensor(train_input)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_mask = torch.tensor(train_mask)\n",
    "\n",
    "validation_input = torch.tensor(validation_input)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_mask = torch.tensor(validation_mask)\n",
    "\n",
    "test_input = torch.tensor(test_input)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_mask = torch.tensor(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1\n",
    "\n",
    "train_data= TensorDataset(train_input, train_mask, train_labels)\n",
    "train_sampler= RandomSampler(train_data)\n",
    "train_dataloader= DataLoader(train_data, sampler= train_sampler, batch_size=batch_size)\n",
    "\n",
    "valid_data= TensorDataset(validation_input, validation_mask, validation_labels)\n",
    "valid_sampler= RandomSampler(valid_data)\n",
    "validation_dataloader= DataLoader(train_data, sampler= valid_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data= TensorDataset(test_input, test_mask, test_labels)\n",
    "test_sampler= RandomSampler(test_data)\n",
    "test_dataloader= DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "==== Embedding Layer ====\n",
      "bert.embeddings.word_embeddings.weight                       (105879, 768)\n",
      "bert.embeddings.position_embeddings.weight                     (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                                   (768,)\n",
      "bert.embeddings.LayerNorm.bias                                     (768,)\n",
      "==== First Transformers ====\n",
      "bert.encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                             (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "==== Output Layer ====\n",
      "bert.pooler.dense.weight                                       (768, 768)\n",
      "bert.pooler.dense.bias                                             (768,)\n",
      "classifier.weight                                                (3, 768)\n",
      "classifier.bias                                                      (3,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print(\"The BERT model has {:} different named parameters.\".format(len(params)))\n",
    "\n",
    "print(\"==== Embedding Layer ====\")\n",
    "for p in params[0:5]:\n",
    "  print(\"{:<60} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print(\"==== First Transformers ====\")\n",
    "for p in params[5:21]:\n",
    "  print(\"{:<60} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print(\"==== Output Layer ====\")\n",
    "for p in params[-4:]:\n",
    "  print(\"{:<60} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\NLP\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 2e-5,\n",
    "    eps = 1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scheduler untuk tau berapa total step\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "#jumlah batch x epoch\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                             num_warmup_steps = 0,\n",
    "                                             num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "  elapsed_rounded = int(round(elapsed))\n",
    "  return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Epoch 1 / 10 =======\n",
      "Training...\n",
      "Batch    40 of 38,250.     Elapsed: 0:00:10\n",
      "Batch    80 of 38,250.     Elapsed: 0:00:19\n",
      "Batch   120 of 38,250.     Elapsed: 0:00:27\n",
      "Batch   160 of 38,250.     Elapsed: 0:00:36\n",
      "Batch   200 of 38,250.     Elapsed: 0:00:44\n",
      "Batch   240 of 38,250.     Elapsed: 0:00:52\n",
      "Batch   280 of 38,250.     Elapsed: 0:01:01\n",
      "Batch   320 of 38,250.     Elapsed: 0:01:09\n",
      "Batch   360 of 38,250.     Elapsed: 0:01:18\n",
      "Batch   400 of 38,250.     Elapsed: 0:01:26\n",
      "Batch   440 of 38,250.     Elapsed: 0:01:35\n",
      "Batch   480 of 38,250.     Elapsed: 0:01:43\n",
      "Batch   520 of 38,250.     Elapsed: 0:01:52\n",
      "Batch   560 of 38,250.     Elapsed: 0:02:00\n",
      "Batch   600 of 38,250.     Elapsed: 0:02:09\n",
      "Batch   640 of 38,250.     Elapsed: 0:02:17\n",
      "Batch   680 of 38,250.     Elapsed: 0:02:25\n",
      "Batch   720 of 38,250.     Elapsed: 0:02:34\n",
      "Batch   760 of 38,250.     Elapsed: 0:02:42\n",
      "Batch   800 of 38,250.     Elapsed: 0:02:51\n",
      "Batch   840 of 38,250.     Elapsed: 0:02:59\n",
      "Batch   880 of 38,250.     Elapsed: 0:03:08\n",
      "Batch   920 of 38,250.     Elapsed: 0:03:16\n",
      "Batch   960 of 38,250.     Elapsed: 0:03:25\n",
      "Batch 1,000 of 38,250.     Elapsed: 0:03:33\n",
      "Batch 1,040 of 38,250.     Elapsed: 0:03:42\n",
      "Batch 1,080 of 38,250.     Elapsed: 0:03:50\n",
      "Batch 1,120 of 38,250.     Elapsed: 0:03:59\n",
      "Batch 1,160 of 38,250.     Elapsed: 0:04:07\n",
      "Batch 1,200 of 38,250.     Elapsed: 0:04:16\n",
      "Batch 1,240 of 38,250.     Elapsed: 0:04:24\n",
      "Batch 1,280 of 38,250.     Elapsed: 0:04:33\n",
      "Batch 1,320 of 38,250.     Elapsed: 0:04:41\n",
      "Batch 1,360 of 38,250.     Elapsed: 0:04:50\n",
      "Batch 1,400 of 38,250.     Elapsed: 0:04:58\n",
      "Batch 1,440 of 38,250.     Elapsed: 0:05:07\n",
      "Batch 1,480 of 38,250.     Elapsed: 0:05:15\n",
      "Batch 1,520 of 38,250.     Elapsed: 0:05:24\n",
      "Batch 1,560 of 38,250.     Elapsed: 0:05:32\n",
      "Batch 1,600 of 38,250.     Elapsed: 0:05:41\n",
      "Batch 1,640 of 38,250.     Elapsed: 0:05:49\n",
      "Batch 1,680 of 38,250.     Elapsed: 0:05:57\n",
      "Batch 1,720 of 38,250.     Elapsed: 0:06:06\n",
      "Batch 1,760 of 38,250.     Elapsed: 0:06:14\n",
      "Batch 1,800 of 38,250.     Elapsed: 0:06:23\n",
      "Batch 1,840 of 38,250.     Elapsed: 0:06:31\n",
      "Batch 1,880 of 38,250.     Elapsed: 0:06:40\n",
      "Batch 1,920 of 38,250.     Elapsed: 0:06:48\n",
      "Batch 1,960 of 38,250.     Elapsed: 0:06:57\n",
      "Batch 2,000 of 38,250.     Elapsed: 0:07:05\n",
      "Batch 2,040 of 38,250.     Elapsed: 0:07:14\n",
      "Batch 2,080 of 38,250.     Elapsed: 0:07:22\n",
      "Batch 2,120 of 38,250.     Elapsed: 0:07:31\n",
      "Batch 2,160 of 38,250.     Elapsed: 0:07:39\n",
      "Batch 2,200 of 38,250.     Elapsed: 0:07:48\n",
      "Batch 2,240 of 38,250.     Elapsed: 0:07:56\n",
      "Batch 2,280 of 38,250.     Elapsed: 0:08:05\n",
      "Batch 2,320 of 38,250.     Elapsed: 0:08:13\n",
      "Batch 2,360 of 38,250.     Elapsed: 0:08:22\n",
      "Batch 2,400 of 38,250.     Elapsed: 0:08:30\n",
      "Batch 2,440 of 38,250.     Elapsed: 0:08:39\n",
      "Batch 2,480 of 38,250.     Elapsed: 0:08:47\n",
      "Batch 2,520 of 38,250.     Elapsed: 0:08:56\n",
      "Batch 2,560 of 38,250.     Elapsed: 0:09:04\n",
      "Batch 2,600 of 38,250.     Elapsed: 0:09:13\n",
      "Batch 2,640 of 38,250.     Elapsed: 0:09:21\n",
      "Batch 2,680 of 38,250.     Elapsed: 0:09:30\n",
      "Batch 2,720 of 38,250.     Elapsed: 0:09:38\n",
      "Batch 2,760 of 38,250.     Elapsed: 0:09:47\n",
      "Batch 2,800 of 38,250.     Elapsed: 0:09:55\n",
      "Batch 2,840 of 38,250.     Elapsed: 0:10:04\n",
      "Batch 2,880 of 38,250.     Elapsed: 0:10:12\n",
      "Batch 2,920 of 38,250.     Elapsed: 0:10:21\n",
      "Batch 2,960 of 38,250.     Elapsed: 0:10:29\n",
      "Batch 3,000 of 38,250.     Elapsed: 0:10:38\n",
      "Batch 3,040 of 38,250.     Elapsed: 0:10:46\n",
      "Batch 3,080 of 38,250.     Elapsed: 0:10:55\n",
      "Batch 3,120 of 38,250.     Elapsed: 0:11:03\n",
      "Batch 3,160 of 38,250.     Elapsed: 0:11:12\n",
      "Batch 3,200 of 38,250.     Elapsed: 0:11:20\n",
      "Batch 3,240 of 38,250.     Elapsed: 0:11:29\n",
      "Batch 3,280 of 38,250.     Elapsed: 0:11:37\n",
      "Batch 3,320 of 38,250.     Elapsed: 0:11:46\n",
      "Batch 3,360 of 38,250.     Elapsed: 0:11:54\n",
      "Batch 3,400 of 38,250.     Elapsed: 0:12:03\n",
      "Batch 3,440 of 38,250.     Elapsed: 0:12:12\n",
      "Batch 3,480 of 38,250.     Elapsed: 0:12:20\n",
      "Batch 3,520 of 38,250.     Elapsed: 0:12:29\n",
      "Batch 3,560 of 38,250.     Elapsed: 0:12:37\n",
      "Batch 3,600 of 38,250.     Elapsed: 0:12:46\n",
      "Batch 3,640 of 38,250.     Elapsed: 0:12:54\n",
      "Batch 3,680 of 38,250.     Elapsed: 0:13:03\n",
      "Batch 3,720 of 38,250.     Elapsed: 0:13:11\n",
      "Batch 3,760 of 38,250.     Elapsed: 0:13:20\n",
      "Batch 3,800 of 38,250.     Elapsed: 0:13:28\n",
      "Batch 3,840 of 38,250.     Elapsed: 0:13:37\n",
      "Batch 3,880 of 38,250.     Elapsed: 0:13:45\n",
      "Batch 3,920 of 38,250.     Elapsed: 0:13:54\n",
      "Batch 3,960 of 38,250.     Elapsed: 0:14:02\n",
      "Batch 4,000 of 38,250.     Elapsed: 0:14:11\n",
      "Batch 4,040 of 38,250.     Elapsed: 0:14:19\n",
      "Batch 4,080 of 38,250.     Elapsed: 0:14:28\n",
      "Batch 4,120 of 38,250.     Elapsed: 0:14:36\n",
      "Batch 4,160 of 38,250.     Elapsed: 0:14:45\n",
      "Batch 4,200 of 38,250.     Elapsed: 0:14:53\n",
      "Batch 4,240 of 38,250.     Elapsed: 0:15:02\n",
      "Batch 4,280 of 38,250.     Elapsed: 0:15:10\n",
      "Batch 4,320 of 38,250.     Elapsed: 0:15:19\n",
      "Batch 4,360 of 38,250.     Elapsed: 0:15:27\n",
      "Batch 4,400 of 38,250.     Elapsed: 0:15:36\n",
      "Batch 4,440 of 38,250.     Elapsed: 0:15:44\n",
      "Batch 4,480 of 38,250.     Elapsed: 0:15:53\n",
      "Batch 4,520 of 38,250.     Elapsed: 0:16:01\n",
      "Batch 4,560 of 38,250.     Elapsed: 0:16:10\n",
      "Batch 4,600 of 38,250.     Elapsed: 0:16:18\n",
      "Batch 4,640 of 38,250.     Elapsed: 0:16:27\n",
      "Batch 4,680 of 38,250.     Elapsed: 0:16:35\n",
      "Batch 4,720 of 38,250.     Elapsed: 0:16:44\n",
      "Batch 4,760 of 38,250.     Elapsed: 0:16:52\n",
      "Batch 4,800 of 38,250.     Elapsed: 0:17:01\n",
      "Batch 4,840 of 38,250.     Elapsed: 0:17:10\n",
      "Batch 4,880 of 38,250.     Elapsed: 0:17:18\n",
      "Batch 4,920 of 38,250.     Elapsed: 0:17:27\n",
      "Batch 4,960 of 38,250.     Elapsed: 0:17:35\n",
      "Batch 5,000 of 38,250.     Elapsed: 0:17:44\n",
      "Batch 5,040 of 38,250.     Elapsed: 0:17:52\n",
      "Batch 5,080 of 38,250.     Elapsed: 0:18:01\n",
      "Batch 5,120 of 38,250.     Elapsed: 0:18:09\n",
      "Batch 5,160 of 38,250.     Elapsed: 0:18:18\n",
      "Batch 5,200 of 38,250.     Elapsed: 0:18:26\n",
      "Batch 5,240 of 38,250.     Elapsed: 0:18:35\n",
      "Batch 5,280 of 38,250.     Elapsed: 0:18:43\n",
      "Batch 5,320 of 38,250.     Elapsed: 0:18:52\n",
      "Batch 5,360 of 38,250.     Elapsed: 0:19:00\n",
      "Batch 5,400 of 38,250.     Elapsed: 0:19:09\n",
      "Batch 5,440 of 38,250.     Elapsed: 0:19:17\n",
      "Batch 5,480 of 38,250.     Elapsed: 0:19:26\n",
      "Batch 5,520 of 38,250.     Elapsed: 0:19:34\n",
      "Batch 5,560 of 38,250.     Elapsed: 0:19:43\n",
      "Batch 5,600 of 38,250.     Elapsed: 0:19:51\n",
      "Batch 5,640 of 38,250.     Elapsed: 0:20:00\n",
      "Batch 5,680 of 38,250.     Elapsed: 0:20:08\n",
      "Batch 5,720 of 38,250.     Elapsed: 0:20:17\n",
      "Batch 5,760 of 38,250.     Elapsed: 0:20:25\n",
      "Batch 5,800 of 38,250.     Elapsed: 0:20:34\n",
      "Batch 5,840 of 38,250.     Elapsed: 0:20:42\n",
      "Batch 5,880 of 38,250.     Elapsed: 0:20:51\n",
      "Batch 5,920 of 38,250.     Elapsed: 0:20:59\n",
      "Batch 5,960 of 38,250.     Elapsed: 0:21:08\n",
      "Batch 6,000 of 38,250.     Elapsed: 0:21:16\n",
      "Batch 6,040 of 38,250.     Elapsed: 0:21:25\n",
      "Batch 6,080 of 38,250.     Elapsed: 0:21:33\n",
      "Batch 6,120 of 38,250.     Elapsed: 0:21:42\n",
      "Batch 6,160 of 38,250.     Elapsed: 0:21:51\n",
      "Batch 6,200 of 38,250.     Elapsed: 0:21:59\n",
      "Batch 6,240 of 38,250.     Elapsed: 0:22:08\n",
      "Batch 6,280 of 38,250.     Elapsed: 0:22:16\n",
      "Batch 6,320 of 38,250.     Elapsed: 0:22:25\n",
      "Batch 6,360 of 38,250.     Elapsed: 0:22:33\n",
      "Batch 6,400 of 38,250.     Elapsed: 0:22:42\n",
      "Batch 6,440 of 38,250.     Elapsed: 0:22:50\n",
      "Batch 6,480 of 38,250.     Elapsed: 0:22:59\n",
      "Batch 6,520 of 38,250.     Elapsed: 0:23:07\n",
      "Batch 6,560 of 38,250.     Elapsed: 0:23:16\n",
      "Batch 6,600 of 38,250.     Elapsed: 0:23:24\n",
      "Batch 6,640 of 38,250.     Elapsed: 0:23:33\n",
      "Batch 6,680 of 38,250.     Elapsed: 0:23:41\n",
      "Batch 6,720 of 38,250.     Elapsed: 0:23:50\n",
      "Batch 6,760 of 38,250.     Elapsed: 0:23:58\n",
      "Batch 6,800 of 38,250.     Elapsed: 0:24:07\n",
      "Batch 6,840 of 38,250.     Elapsed: 0:24:15\n",
      "Batch 6,880 of 38,250.     Elapsed: 0:24:24\n",
      "Batch 6,920 of 38,250.     Elapsed: 0:24:32\n",
      "Batch 6,960 of 38,250.     Elapsed: 0:24:41\n",
      "Batch 7,000 of 38,250.     Elapsed: 0:24:50\n",
      "Batch 7,040 of 38,250.     Elapsed: 0:24:58\n",
      "Batch 7,080 of 38,250.     Elapsed: 0:25:07\n",
      "Batch 7,120 of 38,250.     Elapsed: 0:25:15\n",
      "Batch 7,160 of 38,250.     Elapsed: 0:25:24\n",
      "Batch 7,200 of 38,250.     Elapsed: 0:25:32\n",
      "Batch 7,240 of 38,250.     Elapsed: 0:25:41\n",
      "Batch 7,280 of 38,250.     Elapsed: 0:25:49\n",
      "Batch 7,320 of 38,250.     Elapsed: 0:25:58\n",
      "Batch 7,360 of 38,250.     Elapsed: 0:26:06\n",
      "Batch 7,400 of 38,250.     Elapsed: 0:26:15\n",
      "Batch 7,440 of 38,250.     Elapsed: 0:26:23\n",
      "Batch 7,480 of 38,250.     Elapsed: 0:26:32\n",
      "Batch 7,520 of 38,250.     Elapsed: 0:26:40\n",
      "Batch 7,560 of 38,250.     Elapsed: 0:26:49\n",
      "Batch 7,600 of 38,250.     Elapsed: 0:26:57\n",
      "Batch 7,640 of 38,250.     Elapsed: 0:27:06\n",
      "Batch 7,680 of 38,250.     Elapsed: 0:27:14\n",
      "Batch 7,720 of 38,250.     Elapsed: 0:27:23\n",
      "Batch 7,760 of 38,250.     Elapsed: 0:27:31\n",
      "Batch 7,800 of 38,250.     Elapsed: 0:27:40\n",
      "Batch 7,840 of 38,250.     Elapsed: 0:27:48\n",
      "Batch 7,880 of 38,250.     Elapsed: 0:27:57\n",
      "Batch 7,920 of 38,250.     Elapsed: 0:28:06\n",
      "Batch 7,960 of 38,250.     Elapsed: 0:28:14\n",
      "Batch 8,000 of 38,250.     Elapsed: 0:28:23\n",
      "Batch 8,040 of 38,250.     Elapsed: 0:28:31\n",
      "Batch 8,080 of 38,250.     Elapsed: 0:28:40\n",
      "Batch 8,120 of 38,250.     Elapsed: 0:28:48\n",
      "Batch 8,160 of 38,250.     Elapsed: 0:28:57\n",
      "Batch 8,200 of 38,250.     Elapsed: 0:29:05\n",
      "Batch 8,240 of 38,250.     Elapsed: 0:29:14\n",
      "Batch 8,280 of 38,250.     Elapsed: 0:29:22\n",
      "Batch 8,320 of 38,250.     Elapsed: 0:29:31\n",
      "Batch 8,360 of 38,250.     Elapsed: 0:29:39\n",
      "Batch 8,400 of 38,250.     Elapsed: 0:29:48\n",
      "Batch 8,440 of 38,250.     Elapsed: 0:29:56\n",
      "Batch 8,480 of 38,250.     Elapsed: 0:30:05\n",
      "Batch 8,520 of 38,250.     Elapsed: 0:30:13\n",
      "Batch 8,560 of 38,250.     Elapsed: 0:30:22\n",
      "Batch 8,600 of 38,250.     Elapsed: 0:30:30\n",
      "Batch 8,640 of 38,250.     Elapsed: 0:30:39\n",
      "Batch 8,680 of 38,250.     Elapsed: 0:30:47\n",
      "Batch 8,720 of 38,250.     Elapsed: 0:30:56\n",
      "Batch 8,760 of 38,250.     Elapsed: 0:31:05\n",
      "Batch 8,800 of 38,250.     Elapsed: 0:31:13\n",
      "Batch 8,840 of 38,250.     Elapsed: 0:31:22\n",
      "Batch 8,880 of 38,250.     Elapsed: 0:31:30\n",
      "Batch 8,920 of 38,250.     Elapsed: 0:31:39\n",
      "Batch 8,960 of 38,250.     Elapsed: 0:31:47\n",
      "Batch 9,000 of 38,250.     Elapsed: 0:31:56\n",
      "Batch 9,040 of 38,250.     Elapsed: 0:32:04\n",
      "Batch 9,080 of 38,250.     Elapsed: 0:32:13\n",
      "Batch 9,120 of 38,250.     Elapsed: 0:32:21\n",
      "Batch 9,160 of 38,250.     Elapsed: 0:32:30\n",
      "Batch 9,200 of 38,250.     Elapsed: 0:32:38\n",
      "Batch 9,240 of 38,250.     Elapsed: 0:32:47\n",
      "Batch 9,280 of 38,250.     Elapsed: 0:32:55\n",
      "Batch 9,320 of 38,250.     Elapsed: 0:33:04\n",
      "Batch 9,360 of 38,250.     Elapsed: 0:33:12\n",
      "Batch 9,400 of 38,250.     Elapsed: 0:33:21\n",
      "Batch 9,440 of 38,250.     Elapsed: 0:33:29\n",
      "Batch 9,480 of 38,250.     Elapsed: 0:33:38\n",
      "Batch 9,520 of 38,250.     Elapsed: 0:33:47\n",
      "Batch 9,560 of 38,250.     Elapsed: 0:33:55\n",
      "Batch 9,600 of 38,250.     Elapsed: 0:34:04\n",
      "Batch 9,640 of 38,250.     Elapsed: 0:34:12\n",
      "Batch 9,680 of 38,250.     Elapsed: 0:34:21\n",
      "Batch 9,720 of 38,250.     Elapsed: 0:34:29\n",
      "Batch 9,760 of 38,250.     Elapsed: 0:34:38\n",
      "Batch 9,800 of 38,250.     Elapsed: 0:34:46\n",
      "Batch 9,840 of 38,250.     Elapsed: 0:34:55\n",
      "Batch 9,880 of 38,250.     Elapsed: 0:35:03\n",
      "Batch 9,920 of 38,250.     Elapsed: 0:35:12\n",
      "Batch 9,960 of 38,250.     Elapsed: 0:35:20\n",
      "Batch 10,000 of 38,250.     Elapsed: 0:35:29\n",
      "Batch 10,040 of 38,250.     Elapsed: 0:35:37\n",
      "Batch 10,080 of 38,250.     Elapsed: 0:35:46\n",
      "Batch 10,120 of 38,250.     Elapsed: 0:35:54\n",
      "Batch 10,160 of 38,250.     Elapsed: 0:36:03\n",
      "Batch 10,200 of 38,250.     Elapsed: 0:36:11\n",
      "Batch 10,240 of 38,250.     Elapsed: 0:36:20\n",
      "Batch 10,280 of 38,250.     Elapsed: 0:36:28\n",
      "Batch 10,320 of 38,250.     Elapsed: 0:36:37\n",
      "Batch 10,360 of 38,250.     Elapsed: 0:36:46\n",
      "Batch 10,400 of 38,250.     Elapsed: 0:36:54\n",
      "Batch 10,440 of 38,250.     Elapsed: 0:37:03\n",
      "Batch 10,480 of 38,250.     Elapsed: 0:37:11\n",
      "Batch 10,520 of 38,250.     Elapsed: 0:37:20\n",
      "Batch 10,560 of 38,250.     Elapsed: 0:37:28\n",
      "Batch 10,600 of 38,250.     Elapsed: 0:37:37\n",
      "Batch 10,640 of 38,250.     Elapsed: 0:37:45\n",
      "Batch 10,680 of 38,250.     Elapsed: 0:37:54\n",
      "Batch 10,720 of 38,250.     Elapsed: 0:38:02\n",
      "Batch 10,760 of 38,250.     Elapsed: 0:38:11\n",
      "Batch 10,800 of 38,250.     Elapsed: 0:38:19\n",
      "Batch 10,840 of 38,250.     Elapsed: 0:38:28\n",
      "Batch 10,880 of 38,250.     Elapsed: 0:38:36\n",
      "Batch 10,920 of 38,250.     Elapsed: 0:38:45\n",
      "Batch 10,960 of 38,250.     Elapsed: 0:38:53\n",
      "Batch 11,000 of 38,250.     Elapsed: 0:39:02\n",
      "Batch 11,040 of 38,250.     Elapsed: 0:39:11\n",
      "Batch 11,080 of 38,250.     Elapsed: 0:39:19\n",
      "Batch 11,120 of 38,250.     Elapsed: 0:39:28\n",
      "Batch 11,160 of 38,250.     Elapsed: 0:39:36\n",
      "Batch 11,200 of 38,250.     Elapsed: 0:39:45\n",
      "Batch 11,240 of 38,250.     Elapsed: 0:39:53\n",
      "Batch 11,280 of 38,250.     Elapsed: 0:40:02\n",
      "Batch 11,320 of 38,250.     Elapsed: 0:40:10\n",
      "Batch 11,360 of 38,250.     Elapsed: 0:40:19\n",
      "Batch 11,400 of 38,250.     Elapsed: 0:40:27\n",
      "Batch 11,440 of 38,250.     Elapsed: 0:40:36\n",
      "Batch 11,480 of 38,250.     Elapsed: 0:40:44\n",
      "Batch 11,520 of 38,250.     Elapsed: 0:40:53\n",
      "Batch 11,560 of 38,250.     Elapsed: 0:41:01\n",
      "Batch 11,600 of 38,250.     Elapsed: 0:41:10\n",
      "Batch 11,640 of 38,250.     Elapsed: 0:41:18\n",
      "Batch 11,680 of 38,250.     Elapsed: 0:41:27\n",
      "Batch 11,720 of 38,250.     Elapsed: 0:41:35\n",
      "Batch 11,760 of 38,250.     Elapsed: 0:41:44\n",
      "Batch 11,800 of 38,250.     Elapsed: 0:41:52\n",
      "Batch 11,840 of 38,250.     Elapsed: 0:42:01\n",
      "Batch 11,880 of 38,250.     Elapsed: 0:42:09\n",
      "Batch 11,920 of 38,250.     Elapsed: 0:42:18\n",
      "Batch 11,960 of 38,250.     Elapsed: 0:42:27\n",
      "Batch 12,000 of 38,250.     Elapsed: 0:42:35\n",
      "Batch 12,040 of 38,250.     Elapsed: 0:42:44\n",
      "Batch 12,080 of 38,250.     Elapsed: 0:42:52\n",
      "Batch 12,120 of 38,250.     Elapsed: 0:43:01\n",
      "Batch 12,160 of 38,250.     Elapsed: 0:43:09\n",
      "Batch 12,200 of 38,250.     Elapsed: 0:43:18\n",
      "Batch 12,240 of 38,250.     Elapsed: 0:43:26\n",
      "Batch 12,280 of 38,250.     Elapsed: 0:43:35\n",
      "Batch 12,320 of 38,250.     Elapsed: 0:43:43\n",
      "Batch 12,360 of 38,250.     Elapsed: 0:43:52\n",
      "Batch 12,400 of 38,250.     Elapsed: 0:44:00\n",
      "Batch 12,440 of 38,250.     Elapsed: 0:44:09\n",
      "Batch 12,480 of 38,250.     Elapsed: 0:44:17\n",
      "Batch 12,520 of 38,250.     Elapsed: 0:44:26\n",
      "Batch 12,560 of 38,250.     Elapsed: 0:44:34\n",
      "Batch 12,600 of 38,250.     Elapsed: 0:44:43\n",
      "Batch 12,640 of 38,250.     Elapsed: 0:44:52\n",
      "Batch 12,680 of 38,250.     Elapsed: 0:45:00\n",
      "Batch 12,720 of 38,250.     Elapsed: 0:45:09\n",
      "Batch 12,760 of 38,250.     Elapsed: 0:45:17\n",
      "Batch 12,800 of 38,250.     Elapsed: 0:45:26\n",
      "Batch 12,840 of 38,250.     Elapsed: 0:45:34\n",
      "Batch 12,880 of 38,250.     Elapsed: 0:45:43\n",
      "Batch 12,920 of 38,250.     Elapsed: 0:45:51\n",
      "Batch 12,960 of 38,250.     Elapsed: 0:46:00\n",
      "Batch 13,000 of 38,250.     Elapsed: 0:46:08\n",
      "Batch 13,040 of 38,250.     Elapsed: 0:46:17\n",
      "Batch 13,080 of 38,250.     Elapsed: 0:46:25\n",
      "Batch 13,120 of 38,250.     Elapsed: 0:46:34\n",
      "Batch 13,160 of 38,250.     Elapsed: 0:46:42\n",
      "Batch 13,200 of 38,250.     Elapsed: 0:46:51\n",
      "Batch 13,240 of 38,250.     Elapsed: 0:46:59\n",
      "Batch 13,280 of 38,250.     Elapsed: 0:47:08\n",
      "Batch 13,320 of 38,250.     Elapsed: 0:47:17\n",
      "Batch 13,360 of 38,250.     Elapsed: 0:47:25\n",
      "Batch 13,400 of 38,250.     Elapsed: 0:47:34\n",
      "Batch 13,440 of 38,250.     Elapsed: 0:47:42\n",
      "Batch 13,480 of 38,250.     Elapsed: 0:47:51\n",
      "Batch 13,520 of 38,250.     Elapsed: 0:47:59\n",
      "Batch 13,560 of 38,250.     Elapsed: 0:48:08\n",
      "Batch 13,600 of 38,250.     Elapsed: 0:48:16\n",
      "Batch 13,640 of 38,250.     Elapsed: 0:48:25\n",
      "Batch 13,680 of 38,250.     Elapsed: 0:48:33\n",
      "Batch 13,720 of 38,250.     Elapsed: 0:48:42\n",
      "Batch 13,760 of 38,250.     Elapsed: 0:48:50\n",
      "Batch 13,800 of 38,250.     Elapsed: 0:48:59\n",
      "Batch 13,840 of 38,250.     Elapsed: 0:49:07\n",
      "Batch 13,880 of 38,250.     Elapsed: 0:49:16\n",
      "Batch 13,920 of 38,250.     Elapsed: 0:49:24\n",
      "Batch 13,960 of 38,250.     Elapsed: 0:49:33\n",
      "Batch 14,000 of 38,250.     Elapsed: 0:49:42\n",
      "Batch 14,040 of 38,250.     Elapsed: 0:49:50\n",
      "Batch 14,080 of 38,250.     Elapsed: 0:49:59\n",
      "Batch 14,120 of 38,250.     Elapsed: 0:50:07\n",
      "Batch 14,160 of 38,250.     Elapsed: 0:50:16\n",
      "Batch 14,200 of 38,250.     Elapsed: 0:50:24\n",
      "Batch 14,240 of 38,250.     Elapsed: 0:50:33\n",
      "Batch 14,280 of 38,250.     Elapsed: 0:50:41\n",
      "Batch 14,320 of 38,250.     Elapsed: 0:50:50\n",
      "Batch 14,360 of 38,250.     Elapsed: 0:50:58\n",
      "Batch 14,400 of 38,250.     Elapsed: 0:51:07\n",
      "Batch 14,440 of 38,250.     Elapsed: 0:51:15\n",
      "Batch 14,480 of 38,250.     Elapsed: 0:51:24\n",
      "Batch 14,520 of 38,250.     Elapsed: 0:51:32\n",
      "Batch 14,560 of 38,250.     Elapsed: 0:51:41\n",
      "Batch 14,600 of 38,250.     Elapsed: 0:51:49\n",
      "Batch 14,640 of 38,250.     Elapsed: 0:51:58\n",
      "Batch 14,680 of 38,250.     Elapsed: 0:52:06\n",
      "Batch 14,720 of 38,250.     Elapsed: 0:52:15\n",
      "Batch 14,760 of 38,250.     Elapsed: 0:52:23\n",
      "Batch 14,800 of 38,250.     Elapsed: 0:52:32\n",
      "Batch 14,840 of 38,250.     Elapsed: 0:52:40\n",
      "Batch 14,880 of 38,250.     Elapsed: 0:52:49\n",
      "Batch 14,920 of 38,250.     Elapsed: 0:52:58\n",
      "Batch 14,960 of 38,250.     Elapsed: 0:53:06\n",
      "Batch 15,000 of 38,250.     Elapsed: 0:53:15\n",
      "Batch 15,040 of 38,250.     Elapsed: 0:53:23\n",
      "Batch 15,080 of 38,250.     Elapsed: 0:53:32\n",
      "Batch 15,120 of 38,250.     Elapsed: 0:53:40\n",
      "Batch 15,160 of 38,250.     Elapsed: 0:53:49\n",
      "Batch 15,200 of 38,250.     Elapsed: 0:53:57\n",
      "Batch 15,240 of 38,250.     Elapsed: 0:54:06\n",
      "Batch 15,280 of 38,250.     Elapsed: 0:54:14\n",
      "Batch 15,320 of 38,250.     Elapsed: 0:54:23\n",
      "Batch 15,360 of 38,250.     Elapsed: 0:54:31\n",
      "Batch 15,400 of 38,250.     Elapsed: 0:54:40\n",
      "Batch 15,440 of 38,250.     Elapsed: 0:54:48\n",
      "Batch 15,480 of 38,250.     Elapsed: 0:54:57\n",
      "Batch 15,520 of 38,250.     Elapsed: 0:55:05\n",
      "Batch 15,560 of 38,250.     Elapsed: 0:55:14\n",
      "Batch 15,600 of 38,250.     Elapsed: 0:55:22\n",
      "Batch 15,640 of 38,250.     Elapsed: 0:55:31\n",
      "Batch 15,680 of 38,250.     Elapsed: 0:55:40\n",
      "Batch 15,720 of 38,250.     Elapsed: 0:55:48\n",
      "Batch 15,760 of 38,250.     Elapsed: 0:55:57\n",
      "Batch 15,800 of 38,250.     Elapsed: 0:56:05\n",
      "Batch 15,840 of 38,250.     Elapsed: 0:56:14\n",
      "Batch 15,880 of 38,250.     Elapsed: 0:56:22\n",
      "Batch 15,920 of 38,250.     Elapsed: 0:56:31\n",
      "Batch 15,960 of 38,250.     Elapsed: 0:56:39\n",
      "Batch 16,000 of 38,250.     Elapsed: 0:56:48\n",
      "Batch 16,040 of 38,250.     Elapsed: 0:56:56\n",
      "Batch 16,080 of 38,250.     Elapsed: 0:57:05\n",
      "Batch 16,120 of 38,250.     Elapsed: 0:57:13\n",
      "Batch 16,160 of 38,250.     Elapsed: 0:57:22\n",
      "Batch 16,200 of 38,250.     Elapsed: 0:57:30\n",
      "Batch 16,240 of 38,250.     Elapsed: 0:57:39\n",
      "Batch 16,280 of 38,250.     Elapsed: 0:57:47\n",
      "Batch 16,320 of 38,250.     Elapsed: 0:57:56\n",
      "Batch 16,360 of 38,250.     Elapsed: 0:58:04\n",
      "Batch 16,400 of 38,250.     Elapsed: 0:58:13\n",
      "Batch 16,440 of 38,250.     Elapsed: 0:58:21\n",
      "Batch 16,480 of 38,250.     Elapsed: 0:58:30\n",
      "Batch 16,520 of 38,250.     Elapsed: 0:58:39\n",
      "Batch 16,560 of 38,250.     Elapsed: 0:58:47\n",
      "Batch 16,600 of 38,250.     Elapsed: 0:58:56\n",
      "Batch 16,640 of 38,250.     Elapsed: 0:59:04\n",
      "Batch 16,680 of 38,250.     Elapsed: 0:59:13\n",
      "Batch 16,720 of 38,250.     Elapsed: 0:59:21\n",
      "Batch 16,760 of 38,250.     Elapsed: 0:59:30\n",
      "Batch 16,800 of 38,250.     Elapsed: 0:59:38\n",
      "Batch 16,840 of 38,250.     Elapsed: 0:59:47\n",
      "Batch 16,880 of 38,250.     Elapsed: 0:59:55\n",
      "Batch 16,920 of 38,250.     Elapsed: 1:00:04\n",
      "Batch 16,960 of 38,250.     Elapsed: 1:00:12\n",
      "Batch 17,000 of 38,250.     Elapsed: 1:00:21\n",
      "Batch 17,040 of 38,250.     Elapsed: 1:00:29\n",
      "Batch 17,080 of 38,250.     Elapsed: 1:00:38\n",
      "Batch 17,120 of 38,250.     Elapsed: 1:00:46\n",
      "Batch 17,160 of 38,250.     Elapsed: 1:00:55\n",
      "Batch 17,200 of 38,250.     Elapsed: 1:01:04\n",
      "Batch 17,240 of 38,250.     Elapsed: 1:01:12\n",
      "Batch 17,280 of 38,250.     Elapsed: 1:01:21\n",
      "Batch 17,320 of 38,250.     Elapsed: 1:01:29\n",
      "Batch 17,360 of 38,250.     Elapsed: 1:01:38\n",
      "Batch 17,400 of 38,250.     Elapsed: 1:01:46\n",
      "Batch 17,440 of 38,250.     Elapsed: 1:01:55\n",
      "Batch 17,480 of 38,250.     Elapsed: 1:02:03\n",
      "Batch 17,520 of 38,250.     Elapsed: 1:02:12\n",
      "Batch 17,560 of 38,250.     Elapsed: 1:02:20\n",
      "Batch 17,600 of 38,250.     Elapsed: 1:02:29\n",
      "Batch 17,640 of 38,250.     Elapsed: 1:02:37\n",
      "Batch 17,680 of 38,250.     Elapsed: 1:02:46\n",
      "Batch 17,720 of 38,250.     Elapsed: 1:02:54\n",
      "Batch 17,760 of 38,250.     Elapsed: 1:03:03\n",
      "Batch 17,800 of 38,250.     Elapsed: 1:03:11\n",
      "Batch 17,840 of 38,250.     Elapsed: 1:03:20\n",
      "Batch 17,880 of 38,250.     Elapsed: 1:03:28\n",
      "Batch 17,920 of 38,250.     Elapsed: 1:03:37\n",
      "Batch 17,960 of 38,250.     Elapsed: 1:03:46\n",
      "Batch 18,000 of 38,250.     Elapsed: 1:03:54\n",
      "Batch 18,040 of 38,250.     Elapsed: 1:04:03\n",
      "Batch 18,080 of 38,250.     Elapsed: 1:04:11\n",
      "Batch 18,120 of 38,250.     Elapsed: 1:04:20\n",
      "Batch 18,160 of 38,250.     Elapsed: 1:04:28\n",
      "Batch 18,200 of 38,250.     Elapsed: 1:04:37\n",
      "Batch 18,240 of 38,250.     Elapsed: 1:04:45\n",
      "Batch 18,280 of 38,250.     Elapsed: 1:04:54\n",
      "Batch 18,320 of 38,250.     Elapsed: 1:05:02\n",
      "Batch 18,360 of 38,250.     Elapsed: 1:05:11\n",
      "Batch 18,400 of 38,250.     Elapsed: 1:05:19\n",
      "Batch 18,440 of 38,250.     Elapsed: 1:05:28\n",
      "Batch 18,480 of 38,250.     Elapsed: 1:05:36\n",
      "Batch 18,520 of 38,250.     Elapsed: 1:05:45\n",
      "Batch 18,560 of 38,250.     Elapsed: 1:05:53\n",
      "Batch 18,600 of 38,250.     Elapsed: 1:06:02\n",
      "Batch 18,640 of 38,250.     Elapsed: 1:06:10\n",
      "Batch 18,680 of 38,250.     Elapsed: 1:06:19\n",
      "Batch 18,720 of 38,250.     Elapsed: 1:06:28\n",
      "Batch 18,760 of 38,250.     Elapsed: 1:06:36\n",
      "Batch 18,800 of 38,250.     Elapsed: 1:06:45\n",
      "Batch 18,840 of 38,250.     Elapsed: 1:06:53\n",
      "Batch 18,880 of 38,250.     Elapsed: 1:07:02\n",
      "Batch 18,920 of 38,250.     Elapsed: 1:07:10\n",
      "Batch 18,960 of 38,250.     Elapsed: 1:07:19\n",
      "Batch 19,000 of 38,250.     Elapsed: 1:07:27\n",
      "Batch 19,040 of 38,250.     Elapsed: 1:07:36\n",
      "Batch 19,080 of 38,250.     Elapsed: 1:07:44\n",
      "Batch 19,120 of 38,250.     Elapsed: 1:07:53\n",
      "Batch 19,160 of 38,250.     Elapsed: 1:08:01\n",
      "Batch 19,200 of 38,250.     Elapsed: 1:08:10\n",
      "Batch 19,240 of 38,250.     Elapsed: 1:08:18\n",
      "Batch 19,280 of 38,250.     Elapsed: 1:08:27\n",
      "Batch 19,320 of 38,250.     Elapsed: 1:08:36\n",
      "Batch 19,360 of 38,250.     Elapsed: 1:08:44\n",
      "Batch 19,400 of 38,250.     Elapsed: 1:08:53\n",
      "Batch 19,440 of 38,250.     Elapsed: 1:09:01\n",
      "Batch 19,480 of 38,250.     Elapsed: 1:09:10\n",
      "Batch 19,520 of 38,250.     Elapsed: 1:09:18\n",
      "Batch 19,560 of 38,250.     Elapsed: 1:09:27\n",
      "Batch 19,600 of 38,250.     Elapsed: 1:09:35\n",
      "Batch 19,640 of 38,250.     Elapsed: 1:09:44\n",
      "Batch 19,680 of 38,250.     Elapsed: 1:09:52\n",
      "Batch 19,720 of 38,250.     Elapsed: 1:10:01\n",
      "Batch 19,760 of 38,250.     Elapsed: 1:10:09\n",
      "Batch 19,800 of 38,250.     Elapsed: 1:10:18\n",
      "Batch 19,840 of 38,250.     Elapsed: 1:10:26\n",
      "Batch 19,880 of 38,250.     Elapsed: 1:10:35\n",
      "Batch 19,920 of 38,250.     Elapsed: 1:10:43\n",
      "Batch 19,960 of 38,250.     Elapsed: 1:10:52\n",
      "Batch 20,000 of 38,250.     Elapsed: 1:11:00\n",
      "Batch 20,040 of 38,250.     Elapsed: 1:11:09\n",
      "Batch 20,080 of 38,250.     Elapsed: 1:11:18\n",
      "Batch 20,120 of 38,250.     Elapsed: 1:11:26\n",
      "Batch 20,160 of 38,250.     Elapsed: 1:11:35\n",
      "Batch 20,200 of 38,250.     Elapsed: 1:11:43\n",
      "Batch 20,240 of 38,250.     Elapsed: 1:11:52\n",
      "Batch 20,280 of 38,250.     Elapsed: 1:12:00\n",
      "Batch 20,320 of 38,250.     Elapsed: 1:12:09\n",
      "Batch 20,360 of 38,250.     Elapsed: 1:12:17\n",
      "Batch 20,400 of 38,250.     Elapsed: 1:12:26\n",
      "Batch 20,440 of 38,250.     Elapsed: 1:12:34\n",
      "Batch 20,480 of 38,250.     Elapsed: 1:12:43\n",
      "Batch 20,520 of 38,250.     Elapsed: 1:12:51\n",
      "Batch 20,560 of 38,250.     Elapsed: 1:13:00\n",
      "Batch 20,600 of 38,250.     Elapsed: 1:13:08\n",
      "Batch 20,640 of 38,250.     Elapsed: 1:13:17\n",
      "Batch 20,680 of 38,250.     Elapsed: 1:13:26\n",
      "Batch 20,720 of 38,250.     Elapsed: 1:13:34\n",
      "Batch 20,760 of 38,250.     Elapsed: 1:13:43\n",
      "Batch 20,800 of 38,250.     Elapsed: 1:13:51\n",
      "Batch 20,840 of 38,250.     Elapsed: 1:14:00\n",
      "Batch 20,880 of 38,250.     Elapsed: 1:14:08\n",
      "Batch 20,920 of 38,250.     Elapsed: 1:14:17\n",
      "Batch 20,960 of 38,250.     Elapsed: 1:14:25\n",
      "Batch 21,000 of 38,250.     Elapsed: 1:14:34\n",
      "Batch 21,040 of 38,250.     Elapsed: 1:14:42\n",
      "Batch 21,080 of 38,250.     Elapsed: 1:14:51\n",
      "Batch 21,120 of 38,250.     Elapsed: 1:15:00\n",
      "Batch 21,160 of 38,250.     Elapsed: 1:15:08\n",
      "Batch 21,200 of 38,250.     Elapsed: 1:15:17\n",
      "Batch 21,240 of 38,250.     Elapsed: 1:15:25\n",
      "Batch 21,280 of 38,250.     Elapsed: 1:15:34\n",
      "Batch 21,320 of 38,250.     Elapsed: 1:15:42\n",
      "Batch 21,360 of 38,250.     Elapsed: 1:15:51\n",
      "Batch 21,400 of 38,250.     Elapsed: 1:15:59\n",
      "Batch 21,440 of 38,250.     Elapsed: 1:16:08\n",
      "Batch 21,480 of 38,250.     Elapsed: 1:16:16\n",
      "Batch 21,520 of 38,250.     Elapsed: 1:16:25\n",
      "Batch 21,560 of 38,250.     Elapsed: 1:16:33\n",
      "Batch 21,600 of 38,250.     Elapsed: 1:16:42\n",
      "Batch 21,640 of 38,250.     Elapsed: 1:16:51\n",
      "Batch 21,680 of 38,250.     Elapsed: 1:16:59\n",
      "Batch 21,720 of 38,250.     Elapsed: 1:17:08\n",
      "Batch 21,760 of 38,250.     Elapsed: 1:17:16\n",
      "Batch 21,800 of 38,250.     Elapsed: 1:17:25\n",
      "Batch 21,840 of 38,250.     Elapsed: 1:17:33\n",
      "Batch 21,880 of 38,250.     Elapsed: 1:17:42\n",
      "Batch 21,920 of 38,250.     Elapsed: 1:17:50\n",
      "Batch 21,960 of 38,250.     Elapsed: 1:17:59\n",
      "Batch 22,000 of 38,250.     Elapsed: 1:18:07\n",
      "Batch 22,040 of 38,250.     Elapsed: 1:18:16\n",
      "Batch 22,080 of 38,250.     Elapsed: 1:18:25\n",
      "Batch 22,120 of 38,250.     Elapsed: 1:18:33\n",
      "Batch 22,160 of 38,250.     Elapsed: 1:18:42\n",
      "Batch 22,200 of 38,250.     Elapsed: 1:18:50\n",
      "Batch 22,240 of 38,250.     Elapsed: 1:18:59\n",
      "Batch 22,280 of 38,250.     Elapsed: 1:19:07\n",
      "Batch 22,320 of 38,250.     Elapsed: 1:19:16\n",
      "Batch 22,360 of 38,250.     Elapsed: 1:19:24\n",
      "Batch 22,400 of 38,250.     Elapsed: 1:19:33\n",
      "Batch 22,440 of 38,250.     Elapsed: 1:19:41\n",
      "Batch 22,480 of 38,250.     Elapsed: 1:19:50\n",
      "Batch 22,520 of 38,250.     Elapsed: 1:19:58\n",
      "Batch 22,560 of 38,250.     Elapsed: 1:20:07\n",
      "Batch 22,600 of 38,250.     Elapsed: 1:20:15\n",
      "Batch 22,640 of 38,250.     Elapsed: 1:20:24\n",
      "Batch 22,680 of 38,250.     Elapsed: 1:20:32\n",
      "Batch 22,720 of 38,250.     Elapsed: 1:20:41\n",
      "Batch 22,760 of 38,250.     Elapsed: 1:20:50\n",
      "Batch 22,800 of 38,250.     Elapsed: 1:20:58\n",
      "Batch 22,840 of 38,250.     Elapsed: 1:21:07\n",
      "Batch 22,880 of 38,250.     Elapsed: 1:21:15\n",
      "Batch 22,920 of 38,250.     Elapsed: 1:21:24\n",
      "Batch 22,960 of 38,250.     Elapsed: 1:21:32\n",
      "Batch 23,000 of 38,250.     Elapsed: 1:21:41\n",
      "Batch 23,040 of 38,250.     Elapsed: 1:21:49\n",
      "Batch 23,080 of 38,250.     Elapsed: 1:21:58\n",
      "Batch 23,120 of 38,250.     Elapsed: 1:22:06\n",
      "Batch 23,160 of 38,250.     Elapsed: 1:22:15\n",
      "Batch 23,200 of 38,250.     Elapsed: 1:22:23\n",
      "Batch 23,240 of 38,250.     Elapsed: 1:22:32\n",
      "Batch 23,280 of 38,250.     Elapsed: 1:22:41\n",
      "Batch 23,320 of 38,250.     Elapsed: 1:22:49\n",
      "Batch 23,360 of 38,250.     Elapsed: 1:22:58\n",
      "Batch 23,400 of 38,250.     Elapsed: 1:23:06\n",
      "Batch 23,440 of 38,250.     Elapsed: 1:23:15\n",
      "Batch 23,480 of 38,250.     Elapsed: 1:23:23\n",
      "Batch 23,520 of 38,250.     Elapsed: 1:23:32\n",
      "Batch 23,560 of 38,250.     Elapsed: 1:23:40\n",
      "Batch 23,600 of 38,250.     Elapsed: 1:23:49\n",
      "Batch 23,640 of 38,250.     Elapsed: 1:23:57\n",
      "Batch 23,680 of 38,250.     Elapsed: 1:24:06\n",
      "Batch 23,720 of 38,250.     Elapsed: 1:24:14\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "  # ===================================\n",
    "  #              Training\n",
    "  # ===================================\n",
    "\n",
    "  print(\"======= Epoch {:} / {:} =======\".format(epoch_i+1, epochs))\n",
    "  print(\"Training...\")\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  # For each batch of training data\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Progress update every 40 batches\n",
    "    if step % 40 == 0 and not step == 0:\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "\n",
    "      print(\"Batch {:>5,} of {:>5,}.     Elapsed: {:}\".format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    b_labels = b_labels.type(torch.long)\n",
    "\n",
    "    \n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    outputs = model(b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "  avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "  loss_values.append(avg_train_loss)\n",
    "\n",
    "  print(\"   Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "  print(\"   Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "  # ===================================\n",
    "  #             Validation\n",
    "  # ===================================\n",
    "\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  for batch in validation_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids,\n",
    "                      token_type_ids=None,\n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    b_labels = b_labels.type(torch.long)\n",
    "\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"   Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "  print(\"   Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
